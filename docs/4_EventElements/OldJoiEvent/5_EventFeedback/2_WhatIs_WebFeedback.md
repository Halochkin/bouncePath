# WhatIs: Feedback on the web

## Physical feedback

> Physical feedback is truly immediate, ie. non-mediated *and* fast.

The first form of feedback a web interface gives us is physical: **hardware feedback**. Until machine interfaces can read our thoughts directly, we need to *physically* interact with the machine via touch, talk, or gestures. 

Primarily, physical user actions control the machine. But, the user can also observe the same actions with his/her own senses: if you say something to a smart speaker, you will hear your own words as they are uttered; if you click on a key, you can both hear a "click" sound and feel the plastic surface of the key. 

Physical feedback is sometimes enough. Seeing your own finger move across a smartphone screen suffices: the browser doesn't need to add a "touch cursor" hidden under your fingertip. When you tap on a keyboard, you hear when keys are hit: the browser doesn't have to add a "tap" sound.

## Software feedback

But, physical feedback is often inadequate. Often, the physical actions a user makes to control the system, do not produce precise enough feedback to guide the user's action. In such circumstances, either a) the browser's native function that control user input, b) platform scripts such composed events and EventSequences, and/or c) app scripts, must give their users **feedback about their inner state**. This we call software feedback.

Software feedback can be given in three different modalities:
1. VisualFeedback: show and change a visual element. 
2. AudioFeedback: play a sound or message.
3. VibrationFeedback: vibrate phone, console controller, etc.

In this chapter we will look at these three different modes of feedback in more detail. But, before this point, we need to describe a) what the feedback is about and b) where the feedback is generated from. 

## What is the feedback about?

**EventSequence state**. We can understand EventSequence state as the state of an ongoing user input gesture. For example, when a user `drag`s an element across the screen, the `drag` EventSequence state includes: a) which element being dragged, b) the current position of the cursor dragging the element, and c) the element the cursor currently hovers. This EventSequence state can both be associated with the DOM (e.g. "which DOM element is being dragged?"), but it can also be DOM context free (e.g. "what is the current `drag` coordinate on screen?").
 
Furthermore, an EventSequence's state can be driven by other properties than user input. For example, EventSequences can handle network interactions (such as "which files can be, is, and has been `load`ed?"), time-based game logic, and other recurring sequences in an app.
 
An EventSequence's state can store and retrieve any data from previous events it has received. 
 
> Composed events cannot make any assumptions about the state of the DOM nor other app specific context.

**App state**. This means state data that not only includes user input actions, but also data from the app and domain logic. For example, if the user fills out a `<form>` in a web shop with a new shipping address, then the app can recognize that the user has never used this particular shipping address before and decide whether a new shipping address is expected (a new user account) or should evoke either a correction or addition to the user's list of shipping addresses.

### Where is the state stored?

The DOM is not the only source of state. Most of an app's state is for example stored in JS objects (for example a Redux `store`). But, also EventSequences may access universal browser state as JS objects such as browsing history that are not available in the DOM (cf. the native EventSequence controlling the `:visited` pseudo-class). 

(todo make chapter about f.x. HoverPreloadLinks, `:visited` augmented by `:preloaded`) (todo make a chapter about a custom EventSequence that uses a universal network resource to for example mark links as "unsafe" that both rely on http vs https, but also on a "universal" blacklist of domains from a server). (todo which other JS objects of the browser state are universal?).

> Both user-, system-, and time-driven events can exist "in the DOM", "on screen", "in the network", "in a game universe", etc. And, individual events can occur in many different contextual landscapes at the same time. 
    
## Where is the feedback generated?

1. **EventSequence function**, both native and custom. Feedback generated by the EventSequence function should only access universal state in an app, such as "the existence of a DOM", but not "a particular DOM structure". This feedback can be reused across many different apps and use-cases.

2. **The app**. Feedback generated by the app has access to app specific data and customizable to app-specific use-cases. This feedback can access most DOM structures, and can for example mutate the text and/or color of a particular DOM element.

## Complexity require feedback

The more complex an EventSequence is, the more room for error and confusion for the user. EventSequence complexity is driven by the number of different steps and the number of elements involved. For example, the drag'n'drop event When an EventSequence requires five different actions, the user will need feedback about:
1. Which actions has so far been registered? For example, did the browser properly register the two finger touch as two separate touches, or were the fingers placed too close together, so as to register only as a single touch?
2. What are the qualities of some of these actions? For example, how far has the mouse been moved so far?
3. Which choices/future actions are currently open to the user? For example, can the user drop a `drag` element over the current `dragover` target?

> The more user actions happening and available, the more need for user feedback.

Composed events and EventSequences are platform level: they extend the generic capabilities of the browser. They can realize gestures that might be conventionalized, but not yet supported by browsers. These functions ability to give feedback is less restricted than we might assume. In this chapter we will show how composed can give their users visual feedback, audio feedback and tactile feedback.

## References

 * []()

## Old draft
New composed event are often related to established UX gestures. For example, a `long-press` composed event might simply generalize a gesture on the OS layer of most smartphones today and transfer it to the web. In such cases, the composed event must echo both the **behavior and feedback conventions** of the OS gesture so that users do not need to *think about* what is happening at the micro level of the interaction. Another example is a composed `pull` event. The pull-down-to-refresh is an established convention in many apps, and if this concept is generalized so as to be applicable to any element, and in all directions, then the user might immediately, or very quickly, translate his existing, automated use of pull-down-to-refresh into a set of other pull-in-a-direction-to-do-something skill.

For example, a touch drag is comprised of the user pressing down on the screen, moving his/her finger in a direction while pressing down, and then lifting up his/her finger. During such a gesture, its state (such as registered finger positions and finger count) changes. Before the finger is pressed down, the gesture is inactive. After the finger is pressed down, the user might start either a click or a drag. While the finger is dragged over the screen, the drag is active. And when the finger is lifted, the gesture is again inactive. The EventSequence must communicate these state changes for the user, it must give some type of feedback.  

A web app on top of a web browser on top of an OS on top of a multipurpose computer, all of which may have a different take on the input signals, can cause problems. Web apps often have complex gestures too. Drag'n'drop combined with custom app logic might require lots of information being passed on to the user. Thus, questions like: did I press the “a” or “caps lock” just now on the keyboard? Did my smartphone register my finger gesture as a click or a drag or a scroll? When hardware feedback is not enough, we need software-mediated feedback.     

> A gesture is a set of actions spread out over time. We implement gestures as EventSequences. 

Both during and after user actions, a web app must make it clear for the user which actions it registered. For example, did the touch hit element A or B? How many pixels has the finger been drag across the screen? Exactly over which element did the user end his drag actions? To answer such questions for the user, both a) the browser's native event functions, b) platform scripts 

